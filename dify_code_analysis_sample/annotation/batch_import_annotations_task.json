{
    "highlights": "The key features in this code are:\n\n1. **Celery Task**: The code defines a Celery shared task called `batch_import_annotations_task` that is responsible for importing a batch of annotations.\n\n2. **Database Operations**: The code performs various database operations, including querying for an `App` instance, creating `MessageAnnotation` and `Document` instances, and committing the changes to the database.\n\n3. **Indexing Annotations**: If the `AppAnnotationSetting` is enabled, the code creates a `Dataset` instance and uses the `Vector` class to index the annotations in the dataset.\n\n4. **Caching and Error Handling**: The code uses Redis to cache the status of the job and the error messages in case of any exceptions.\n\n5. **Logging**: The code extensively uses logging to provide information about the task execution, including the start and end times, and any errors that may occur.\n\nOverall, the key feature of this code is the ability to import a batch of annotations, associate them with the appropriate `App` instance, and index them for search and retrieval purposes, with proper error handling and caching mechanisms.",
    "overall_summary": "This Python file `batch_import_annotations_task.py` contains a Celery task called `batch_import_annotations_task` that is responsible for importing a batch of annotations into the system.\n\nThe key components of the codebase are:\n\n1. **Celery Task**: The `batch_import_annotations_task` function is decorated with `@shared_task(queue='dataset')`, which means it is a Celery task that will be executed in the 'dataset' queue.\n\n2. **Annotation Import**: The task takes the following parameters: `job_id`, `content_list` (a list of dictionaries containing the annotation data), `app_id`, `tenant_id`, and `user_id`. It then processes the `content_list` by creating `MessageAnnotation` objects and `Document` objects for each annotation, and adding them to the database.\n\n3. **Indexing**: If the app's `AppAnnotationSetting` is configured to have annotation indexing enabled, the task will use the `DatasetCollectionBindingService` to get the appropriate `Dataset` object, and then use the `Vector` class to create the index for the new annotations.\n\n4. **Caching and Error Handling**: The task uses Redis to cache the status of the import job, setting a key with a 600-second expiration time. If an error occurs during the import, the task will roll back the database transaction, set an error status in Redis, and log the error message.\n\nOverall, this file is responsible for the batch import of annotations into the system, including creating the necessary database records and indexing the annotations for search and retrieval.",
    "pseudocode": "Sure, here's the high-level pythonic pseudocode for the given code:\n\n```python\n# Define a Celery task to handle batch import of annotations\n@shared_task(queue='dataset')\ndef batch_import_annotations_task(job_id, content_list, app_id, tenant_id, user_id):\n    # Log the start of the task\n    logging.info(f'Start batch import annotation: {job_id}')\n    start_time = time.perf_counter()\n\n    # Set a key in Redis to track the progress of the task\n    indexing_cache_key = f'app_annotation_batch_import_{job_id}'\n\n    # Fetch the app information from the database\n    app = db.session.query(App).filter(\n        App.id == app_id,\n        App.tenant_id == tenant_id,\n        App.status == 'normal'\n    ).first()\n\n    if app:\n        try:\n            # Create a list of Document objects from the content list\n            documents = []\n            for content in content_list:\n                # Create a MessageAnnotation object and add it to the database\n                annotation = MessageAnnotation(\n                    app_id=app.id,\n                    content=content['answer'],\n                    question=content['question'],\n                    account_id=user_id\n                )\n                db.session.add(annotation)\n                db.session.flush()\n\n                # Create a Document object with the annotation metadata\n                document = Document(\n                    page_content=content['question'],\n                    metadata={\n                        \"annotation_id\": annotation.id,\n                        \"app_id\": app_id,\n                        \"doc_id\": annotation.id\n                    }\n                )\n                documents.append(document)\n\n            # Check if annotation reply is enabled, and if so, batch add the annotations to the index\n            app_annotation_setting = db.session.query(AppAnnotationSetting).filter(\n                AppAnnotationSetting.app_id == app_id\n            ).first()\n\n            if app_annotation_setting:\n                # Fetch the dataset collection binding for annotations\n                dataset_collection_binding = DatasetCollectionBindingService.get_dataset_collection_binding_by_id_and_type(\n                    app_annotation_setting.collection_binding_id,\n                    'annotation'\n                )\n                if not dataset_collection_binding:\n                    raise NotFound(\"App annotation setting not found\")\n\n                # Create a new Dataset object and index the documents\n                dataset = Dataset(\n                    id=app_id,\n                    tenant_id=tenant_id,\n                    indexing_technique='high_quality',\n                    embedding_model_provider=dataset_collection_binding.provider_name,\n                    embedding_model=dataset_collection_binding.model_name,\n                    collection_binding_id=dataset_collection_binding.id\n                )\n                vector = Vector(dataset, attributes=['doc_id', 'annotation_id', 'app_id'])\n                vector.create(documents, duplicate_check=True)\n\n            # Commit the database changes and set the Redis key to indicate task completion\n            db.session.commit()\n            redis_client.setex(indexing_cache_key, 600, 'completed')\n            end_time = time.perf_counter()\n            logging.info(f'Build index successful for batch import annotation: {job_id} latency: {end_time - start_time}')\n        except Exception as e:\n            # Roll back the database changes and set the Redis key to indicate an error\n            db.session.rollback()\n            redis_client.setex(indexing_cache_key, 600, 'error')\n            indexing_error_msg_key = f'app_annotation_batch_import_error_msg_{job_id}'\n            redis_client.setex(indexing_error_msg_key, 600, str(e))\n            logging.exception(\"Build index for batch import annotations failed\")\n```\n\nThis pseudocode covers the following high-level logic:\n\n1. The `batch_import_annotations_task` function is defined as a Celery task, which will be executed in the `dataset` queue.\n2. The function logs the start of the task and records the start time.\n3. It sets a Redis key to track the progress of the task.\n4. It fetches the app information from the database based on the provided `app_id` and `tenant_id`.\n5. If the app is found, it creates MessageAnnotation and Document objects from the provided content list, and adds them to the database.\n6. It checks if annotation reply is enabled for the app, and if so, it fetches the dataset collection binding for annotations.\n7. If the dataset collection binding is found, it creates a new Dataset object and indexes the documents using the Vector class.\n8. After the database changes are committed, it sets the Redis key to indicate the task completion.\n9. If any exceptions occur during the process, it rolls back the database changes, sets the Redis key to indicate an error, and logs the exception.\n\nThis pseudocode provides a high-level overview of the task's functionality, including database and Redis operations, error handling, and logging. The actual implementation may include additional details and optimizations specific to the project requirements.",
    "import_relationships": "Imports found:\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom werkzeug.exceptions import NotFound\nfrom core.rag.datasource.vdb.vector_factory import Vector\nfrom core.rag.models.document import Document\nfrom extensions.ext_database import db\nfrom extensions.ext_redis import redis_client\nfrom models.dataset import Dataset\nfrom models.model import App, AppAnnotationSetting, MessageAnnotation\nfrom services.dataset_service import DatasetCollectionBindingService"
}