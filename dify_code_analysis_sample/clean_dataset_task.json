{
    "highlights": "The key features of the code are:\n\n1. **Celery Task**: The code defines a Celery shared task called `clean_dataset_task` that is responsible for cleaning up the dataset when it is deleted.\n\n2. **Database Operations**: The code performs various database operations, including querying for documents and document segments associated with the dataset, deleting those records, and deleting related records in the `DatasetProcessRule`, `DatasetQuery`, and `AppDatasetJoin` tables.\n\n3. **Index Processor**: The code uses an `IndexProcessorFactory` to create an index processor object, which is then used to clean up the index associated with the dataset.\n\n4. **Logging and Timing**: The code logs the start and end of the task, as well as the latency, using the `logging` and `click` modules.\n\n5. **Parameterized Task**: The `clean_dataset_task` function takes several parameters, including the dataset ID, tenant ID, indexing technique, index structure, collection binding ID, and document form. This allows the task to be reused for different datasets with different configurations.\n\nThe key thing to look for in this code is the overall structure of the Celery task, which is responsible for performing the necessary cleanup operations when a dataset is deleted. The task ensures that all related data is removed from the database and the associated index is also cleaned up.",
    "overall_summary": "Here is an overall summary of the provided codebase:\n\nThe codebase consists of a single Python file named `clean_dataset_task.py`. It defines a Celery task function called `clean_dataset_task` that is responsible for cleaning up a dataset when it is deleted.\n\nThe main functionality of the `clean_dataset_task` function is as follows:\n\n1. It takes several parameters as input, including the `dataset_id`, `tenant_id`, `indexing_technique`, `index_struct`, `collection_binding_id`, and `doc_form`.\n2. It retrieves the `Dataset` object for the given `dataset_id` and `tenant_id`.\n3. It retrieves all `Document` and `DocumentSegment` objects associated with the dataset.\n4. If there are any documents associated with the dataset, it uses an `IndexProcessorFactory` to initialize an `IndexProcessor` and then calls the `clean` method of the `IndexProcessor` to clean up the dataset.\n5. It deletes all `Document` and `DocumentSegment` objects associated with the dataset.\n6. It deletes all `DatasetProcessRule`, `DatasetQuery`, and `AppDatasetJoin` objects associated with the dataset.\n7. It commits the changes to the database.\n8. It logs the start and end times of the task, as well as any exceptions that may have occurred.\n\nThe codebase also imports several classes and modules, including:\n- `logging` and `time` from the Python standard library\n- `click` from the Click library\n- `shared_task` from the Celery library\n- `IndexProcessorFactory` from the `core.rag.index_processor.index_processor_factory` module\n- `db` from the `extensions.ext_database` module\n- Several model classes from the `models.dataset` module, including `AppDatasetJoin`, `Dataset`, `DatasetProcessRule`, `DatasetQuery`, `Document`, and `DocumentSegment`.\n\nOverall, this codebase provides a way to clean up a dataset when it is deleted, including deleting all associated documents, segments, and other related objects.",
    "pseudocode": "Here's the high-level pythonic pseudocode for the `clean_dataset_task` function:\n\n```python\n# Function to clean up a dataset when it's deleted\ndef clean_dataset_task(dataset_id, tenant_id, indexing_technique, index_struct, collection_binding_id, doc_form):\n    # Log the start of the task\n    log_info(f\"Start clean dataset when dataset deleted: {dataset_id}\")\n    start_time = get_current_time()\n\n    try:\n        # Create a Dataset instance with the provided information\n        dataset = Dataset(id=dataset_id, tenant_id=tenant_id, indexing_technique=indexing_technique, index_struct=index_struct, collection_binding_id=collection_binding_id)\n\n        # Fetch all documents and segments associated with the dataset\n        documents = query_documents(dataset_id)\n        segments = query_document_segments(dataset_id)\n\n        # If no documents are found, log and return\n        if not documents:\n            log_info(f\"No documents found for dataset: {dataset_id}\")\n            return\n\n        # Initialize the index processor based on the dataset form\n        index_processor = get_index_processor(doc_form)\n\n        # Clean up the index for the dataset\n        index_processor.clean(dataset, None)\n\n        # Delete all documents and segments associated with the dataset\n        delete_documents(documents)\n        delete_segments(segments)\n\n        # Delete all related dataset records (process rules, queries, app-dataset joins)\n        delete_dataset_process_rules(dataset_id)\n        delete_dataset_queries(dataset_id)\n        delete_app_dataset_joins(dataset_id)\n\n        # Commit the changes to the database\n        commit_changes()\n\n        # Log the completion of the task and the latency\n        end_time = get_current_time()\n        log_info(f\"Cleaned dataset when dataset deleted: {dataset_id} | Latency: {end_time - start_time}\")\n    except Exception:\n        # Log the exception if the task fails\n        log_exception(\"Cleaned dataset when dataset deleted failed\")\n```\n\nKey points:\n\n1. The function takes in the necessary parameters to identify the dataset and associated information.\n2. It logs the start of the task and measures the execution time.\n3. The function creates a `Dataset` instance with the provided information.\n4. It fetches all documents and document segments associated with the dataset.\n5. If no documents are found, it logs the information and returns.\n6. It initializes the appropriate index processor based on the dataset form.\n7. The index processor is used to clean up the index for the dataset.\n8. The function then deletes all documents, segments, and related dataset records (process rules, queries, app-dataset joins).\n9. The changes are committed to the database.\n10. Finally, the function logs the completion of the task and the latency.\n11. If any exception occurs during the execution, it logs the exception.\n\nThe pseudocode uses high-level, abstract Python constructs and functions to represent the logic, making it easy to understand the overall flow of the task without getting into the implementation details.",
    "import_relationships": "Imports found:\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom core.rag.index_processor.index_processor_factory import IndexProcessorFactory\nfrom extensions.ext_database import db\nfrom models.dataset import ("
}