{
    "highlights": "The key features of this code are:\n\n1. **Asynchronous Task Execution**: The code uses Celery's `@shared_task` decorator to define an asynchronous task, `document_indexing_sync_task`, which can be executed in the background.\n\n2. **Document Indexing and Updating**: The main purpose of the code is to handle the indexing and updating of documents. It fetches the document from the database, checks if the data source has been updated, and then proceeds to update the document's indexing status and process the document through the `IndexingRunner`.\n\n3. **Notion Integration**: The code handles the case where the document is associated with a Notion page. It extracts the necessary information from the Notion data source and updates the document accordingly.\n\n4. **Error Handling**: The code includes error handling for various scenarios, such as when the document or dataset is not found, or when there are issues during the indexing process.\n\n5. **Logging and Timing**: The code includes logging statements to provide information about the task execution, and measures the time taken to perform various operations.\n\nThe key focus of this code is to ensure that documents are properly indexed and updated based on changes in their data sources, with a particular emphasis on integrating with Notion as a data source.",
    "overall_summary": "The codebase in the provided Python file `document_indexing_sync_task.py` is responsible for asynchronously updating and indexing documents in a document indexing system. Here's a summary of the key components and functionality:\n\n1. **Async Task**: The codebase defines a Celery shared task named `document_indexing_sync_task`. This task is responsible for handling the asynchronous update and indexing of documents.\n\n2. **Document Retrieval**: The task retrieves a document from the database based on the provided `dataset_id` and `document_id`. If the document is not found, a `NotFound` exception is raised.\n\n3. **Notion Extractor**: If the document's data source type is \"notion_import\", the task uses the `NotionExtractor` to retrieve the latest content and metadata of the Notion page associated with the document. It checks if the page has been updated since the last indexing.\n\n4. **Index Cleanup**: If the page has been updated, the task deletes all existing document segments and the associated index entries from the vector index.\n\n5. **Indexing**: After the cleanup, the task triggers the `IndexingRunner` to re-index the document and update the index.\n\n6. **Error Handling**: The task includes exception handling for various scenarios, such as when the document is paused or the indexing process fails.\n\n7. **Logging**: The task uses the `logging` module to log relevant information, such as the start and end of the indexing process, the latency, and any errors that occur.\n\nIn summary, this codebase is responsible for efficiently updating and re-indexing documents in a document indexing system, specifically handling the case of Notion-imported documents. It ensures that the document index is kept up-to-date with the latest changes in the source data.",
    "pseudocode": "Sure, here's the high-level pythonic pseudocode for the given code:\n\n```python\n# Function to handle document indexing and synchronization\n@shared_task(queue='dataset')\ndef document_indexing_sync_task(dataset_id: str, document_id: str):\n    \"\"\"\n    Asynchronously updates a document in the system.\n\n    Args:\n        dataset_id (str): The ID of the dataset the document belongs to.\n        document_id (str): The ID of the document to be updated.\n    \"\"\"\n    # Log the start of the task\n    logging.info(f\"Start sync document: {document_id}\")\n    start_time = time.perf_counter()\n\n    # Fetch the document from the database\n    document = get_document(dataset_id, document_id)\n\n    # Check if the document exists\n    if not document:\n        raise NotFound(\"Document not found\")\n\n    # Determine the data source type and extract relevant information\n    data_source_type = document.data_source_type\n    data_source_info = document.data_source_info_dict\n\n    # Handle Notion-based documents\n    if data_source_type == 'notion_import':\n        handle_notion_document(document, data_source_info)\n\n# Helper function to handle Notion-based documents\ndef handle_notion_document(document, data_source_info):\n    # Extract Notion-specific information from the data source info\n    workspace_id = data_source_info['notion_workspace_id']\n    page_id = data_source_info['notion_page_id']\n    page_type = data_source_info['type']\n    page_edited_time = data_source_info['last_edited_time']\n\n    # Fetch the data source binding for the Notion workspace\n    data_source_binding = get_data_source_binding(document.tenant_id, workspace_id)\n\n    # Create a Notion extractor and get the latest edited time\n    loader = NotionExtractor(workspace_id, page_id, page_type, data_source_binding.access_token, document.tenant_id)\n    last_edited_time = loader.get_notion_last_edited_time()\n\n    # Check if the Notion page has been updated\n    if last_edited_time != page_edited_time:\n        # Update the document's indexing status and processing start time\n        update_document_status(document, 'parsing')\n\n        # Delete all document segments and index entries\n        delete_document_segments_and_index(document.id, document.dataset_id, document.doc_form)\n\n        # Reindex the document\n        reindex_document(document)\n\n# Helper function to fetch a document from the database\ndef get_document(dataset_id: str, document_id: str):\n    return db.session.query(Document).filter(\n        Document.id == document_id,\n        Document.dataset_id == dataset_id\n    ).first()\n\n# Helper function to fetch a data source binding\ndef get_data_source_binding(tenant_id: str, workspace_id: str):\n    return DataSourceBinding.query.filter(\n        db.and_(\n            DataSourceBinding.tenant_id == tenant_id,\n            DataSourceBinding.provider == 'notion',\n            DataSourceBinding.disabled == False,\n            DataSourceBinding.source_info['workspace_id'] == f'\"{workspace_id}\"'\n        )\n    ).first()\n\n# Helper function to update a document's indexing status\ndef update_document_status(document: Document, status: str):\n    document.indexing_status = status\n    document.processing_started_at = datetime.datetime.utcnow()\n    db.session.commit()\n\n# Helper function to delete document segments and index entries\ndef delete_document_segments_and_index(document_id: str, dataset_id: str, index_type: str):\n    dataset = get_dataset(dataset_id)\n    index_processor = IndexProcessorFactory(index_type).init_index_processor()\n\n    segments = get_document_segments(document_id)\n    index_node_ids = [segment.index_node_id for segment in segments]\n\n    # Delete from the vector index\n    index_processor.clean(dataset, index_node_ids)\n\n    for segment in segments:\n        db.session.delete(segment)\n\n# Helper function to reindex a document\ndef reindex_document(document: Document):\n    indexing_runner = IndexingRunner()\n    indexing_runner.run([document])\n```\n\nThis pseudocode provides a high-level overview of the document indexing and synchronization task. It includes the main function `document_indexing_sync_task` and several helper functions to handle various aspects of the task, such as fetching the document, extracting Notion-specific information, updating the document status, deleting segments and index entries, and reindexing the document.\n\nThe pseudocode is written in a Pythonic style, using type annotations, docstrings, and descriptive variable and function names to make the code more readable and understandable.",
    "import_relationships": "Imports found:\nimport datetime\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom werkzeug.exceptions import NotFound\nfrom core.indexing_runner import DocumentIsPausedException, IndexingRunner\nfrom core.rag.extractor.notion_extractor import NotionExtractor\nfrom core.rag.index_processor.index_processor_factory import IndexProcessorFactory\nfrom extensions.ext_database import db\nfrom models.dataset import Dataset, Document, DocumentSegment\nfrom models.source import DataSourceBinding"
}