{
    "highlights": "The key features of this code are:\n\n1. **Celery Task**: The code defines a Celery shared task called `document_indexing_update_task` that is used to asynchronously update a document.\n\n2. **Database Operations**: The code interacts with the database using SQLAlchemy to fetch and update the `Document`, `Dataset`, and `DocumentSegment` models.\n\n3. **Index Cleanup**: When updating a document, the code first deletes all the existing document segments and their associated index nodes from the vector index using the `IndexProcessorFactory` and `IndexingRunner`.\n\n4. **Indexing Execution**: After cleaning up the old index, the code triggers the indexing process for the updated document using the `IndexingRunner`.\n\n5. **Error Handling**: The code handles various exceptions that may occur during the indexing process, such as `NotFound` (document not found), `DocumentIsPausedException`, and any other general exceptions.\n\nThe key purpose of this code is to provide an asynchronous task that can update the indexing of a document in the system, handling the necessary cleanup and re-indexing steps.",
    "overall_summary": "This Python file `document_indexing_update_task.py` contains a Celery task function called `document_indexing_update_task` that is responsible for updating a document's indexing status in a database-backed application.\n\nHere's a summary of the code:\n\n1. The task function `document_indexing_update_task` is a shared Celery task that runs in the 'dataset' queue.\n2. The task takes two parameters: `dataset_id` and `document_id`.\n3. The function first retrieves the `Document` instance from the database based on the `dataset_id` and `document_id`.\n4. If the document is not found, it raises a `NotFound` exception.\n5. The function then updates the `indexing_status` and `processing_started_at` fields of the `Document` instance.\n6. It then deletes all the related `DocumentSegment` instances and removes the corresponding index entries from the index processor.\n7. Finally, it creates an `IndexingRunner` instance and runs the indexing process for the updated document.\n8. The task handles exceptions, such as `DocumentIsPausedException` and other general exceptions, and logs the relevant information.\n\nOverall, this task is responsible for updating the indexing status of a document, deleting the existing index data, and re-indexing the document. This is likely part of a larger document indexing and search system.",
    "pseudocode": "Here's the high-level pythonic pseudocode for the given code:\n\n```python\n# Define a Celery task for asynchronous document indexing update\n@shared_task(queue='dataset')\ndef document_indexing_update_task(dataset_id, document_id):\n    \"\"\"\n    Asynchronously update a document in the indexing process.\n\n    Args:\n        dataset_id (str): The ID of the dataset containing the document.\n        document_id (str): The ID of the document to be updated.\n    \"\"\"\n    # Log the start of the document update process\n    logging.info(f'Start update document: {document_id}')\n    start_time = time.perf_counter()\n\n    # Fetch the document from the database\n    document = db.session.query(Document).filter(\n        Document.id == document_id,\n        Document.dataset_id == dataset_id\n    ).first()\n\n    # If the document is not found, raise a NotFound exception\n    if not document:\n        raise NotFound('Document not found')\n\n    # Update the document's indexing status and processing start time\n    document.indexing_status = 'parsing'\n    document.processing_started_at = datetime.datetime.utcnow()\n    db.session.commit()\n\n    try:\n        # Fetch the dataset associated with the document\n        dataset = db.session.query(Dataset).filter(Dataset.id == dataset_id).first()\n        if not dataset:\n            raise Exception('Dataset not found')\n\n        # Get the index processor based on the document's form\n        index_type = document.doc_form\n        index_processor = IndexProcessorFactory(index_type).init_index_processor()\n\n        # Fetch all the document segments and their index node IDs\n        segments = db.session.query(DocumentSegment).filter(DocumentSegment.document_id == document_id).all()\n        index_node_ids = [segment.index_node_id for segment in segments]\n\n        # Delete the document segments and their index entries\n        index_processor.clean(dataset, index_node_ids)\n        for segment in segments:\n            db.session.delete(segment)\n        db.session.commit()\n        end_time = time.perf_counter()\n        logging.info(f'Cleaned document when document update data source or process rule: {document_id} latency: {end_time - start_time}')\n    except Exception:\n        logging.exception(\"Cleaned document when document update data source or process rule failed\")\n\n    try:\n        # Run the indexing process for the updated document\n        indexing_runner = IndexingRunner()\n        indexing_runner.run([document])\n        end_time = time.perf_counter()\n        logging.info(f'Update document: {document.id} latency: {end_time - start_time}')\n    except DocumentIsPausedException as ex:\n        logging.info(f'{str(ex)}')\n    except Exception:\n        pass\n```\n\nThis pseudocode outlines the high-level logic of the `document_indexing_update_task` function. It first logs the start of the document update process and fetches the document from the database. If the document is not found, it raises a `NotFound` exception.\n\nNext, it updates the document's indexing status and processing start time, then proceeds to delete all the document segments and their corresponding index entries. This is done by fetching the dataset associated with the document, getting the appropriate index processor, and then cleaning the index.\n\nFinally, it runs the indexing process for the updated document using the `IndexingRunner` class. If any exceptions occur during the process, they are caught and logged appropriately.\n\nThe pseudocode is designed to be abstract and informative, providing a high-level overview of the task's functionality without getting into the implementation details.",
    "import_relationships": "Imports found:\nimport datetime\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom werkzeug.exceptions import NotFound\nfrom core.indexing_runner import DocumentIsPausedException, IndexingRunner\nfrom core.rag.index_processor.index_processor_factory import IndexProcessorFactory\nfrom extensions.ext_database import db\nfrom models.dataset import Dataset, Document, DocumentSegment"
}