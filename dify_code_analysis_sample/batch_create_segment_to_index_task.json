{
    "highlights": "The key features of this code are:\n\n1. **Asynchronous Task**: This code is implemented as an asynchronous Celery task, which is executed in the background. The `@shared_task` decorator is used to define the task.\n\n2. **Batch Processing**: The task processes a batch of content items, creating document segments for each item and adding them to the database. This batch approach is likely more efficient than processing one item at a time.\n\n3. **Database and Cache Integration**: The code interacts with the database using SQLAlchemy and stores a completion/error status in Redis for the batch job.\n\n4. **Text Embedding Model**: The code checks the dataset's indexing technique and, if it's set to \"high_quality\", it uses a text embedding model to calculate the number of tokens for each segment.\n\n5. **Error Handling**: The code wraps the main logic in a try-except block to handle any exceptions that may occur during the batch processing, and updates the Redis cache accordingly.\n\nThe key thing to look for in this code is the overall structure and the integration of various components (database, cache, text embedding model) to handle the asynchronous batch processing of document segments.",
    "overall_summary": "This Python file is a Celery task that handles the batch creation of document segments to be indexed. Here's an overall summary of the codebase:\n\n1. **Imports**: The file imports various modules and classes from different packages/modules, including Celery, SQLAlchemy, and custom classes/functions from the application's core and extensions.\n\n2. **batch_create_segment_to_index_task**:\n   - This is a Celery shared task that runs asynchronously in the 'dataset' queue.\n   - The task takes in parameters such as `job_id`, `content` (a list of segments), `dataset_id`, `document_id`, `tenant_id`, and `user_id`.\n   - The task performs the following main operations:\n     - Retrieves the `Dataset` and `Document` instances from the database based on the provided IDs.\n     - Checks the availability of the document (enabled, not archived, indexing status is 'completed').\n     - Processes the content segments and creates `DocumentSegment` instances, including calculating the embeddings using the appropriate text embedding model.\n     - Adds the document segments to the database and commits the changes.\n     - Updates a Redis cache key to indicate the completion or error state of the batch job.\n\n3. **Caching and Indexing**:\n   - The task uses a Redis cache key to keep track of the batch job's status.\n   - It utilizes an `IndexingRunner` class to batch-add the document segments to the index.\n\n4. **Error Handling**:\n   - The task catches any exceptions that may occur during the batch creation process and logs the error.\n   - In case of an error, it updates the Redis cache key to indicate the failure.\n\nIn summary, this codebase is responsible for the asynchronous batch creation and indexing of document segments, leveraging Celery, SQLAlchemy, and custom application components to handle the process efficiently.",
    "pseudocode": "Sure, here's the high-level pythonic pseudocode for the given code:\n\n```python\n# Define a Celery shared task for batch creating segment to index\n@shared_task(queue='dataset')\ndef batch_create_segment_to_index_task(job_id, content, dataset_id, document_id, tenant_id, user_id):\n    \"\"\"\n    Async batch create segment to index\n    \"\"\"\n    # Log the start of the task\n    logging.info(f'Start batch create segment job: {job_id}')\n    start_time = time.perf_counter()\n\n    # Define a cache key for the indexing task\n    indexing_cache_key = f'segment_batch_import_{job_id}'\n\n    try:\n        # Fetch the dataset and document from the database\n        dataset = db.session.query(Dataset).filter(Dataset.id == dataset_id).first()\n        dataset_document = db.session.query(Document).filter(Document.id == document_id).first()\n\n        # Validate the dataset and document\n        if not dataset or not dataset_document or not dataset_document.enabled or dataset_document.archived or dataset_document.indexing_status != 'completed':\n            raise ValueError('Dataset or document not available')\n\n        # Initialize the embedding model if the dataset uses high-quality indexing\n        embedding_model = None\n        if dataset.indexing_technique == 'high_quality':\n            model_manager = ModelManager()\n            embedding_model = model_manager.get_model_instance(\n                tenant_id=dataset.tenant_id,\n                provider=dataset.embedding_model_provider,\n                model_type=ModelType.TEXT_EMBEDDING,\n                model=dataset.embedding_model\n            )\n\n        # Create the document segments and calculate the embeddings\n        document_segments = []\n        model_type_instance = cast(TextEmbeddingModel, embedding_model.model_type_instance)\n        for segment in content:\n            content = segment['content']\n            doc_id = str(uuid.uuid4())\n            segment_hash = helper.generate_text_hash(content)\n            tokens = model_type_instance.get_num_tokens(\n                model=embedding_model.model,\n                credentials=embedding_model.credentials,\n                texts=[content]\n            ) if embedding_model else 0\n            max_position = db.session.query(func.max(DocumentSegment.position)).filter(\n                DocumentSegment.document_id == dataset_document.id\n            ).scalar()\n            segment_document = DocumentSegment(\n                tenant_id=tenant_id,\n                dataset_id=dataset_id,\n                document_id=document_id,\n                index_node_id=doc_id,\n                index_node_hash=segment_hash,\n                position=max_position + 1 if max_position else 1,\n                content=content,\n                word_count=len(content),\n                tokens=tokens,\n                created_by=user_id,\n                indexing_at=datetime.datetime.utcnow(),\n                status='completed',\n                completed_at=datetime.datetime.utcnow()\n            )\n            if dataset_document.doc_form == 'qa_model':\n                segment_document.answer = segment['answer']\n            document_segments.append(segment_document)\n\n        # Add the segments to the database and index them\n        indexing_runner = IndexingRunner()\n        indexing_runner.batch_add_segments(document_segments, dataset)\n        db.session.commit()\n\n        # Set the cache key to indicate the task has completed\n        redis_client.setex(indexing_cache_key, 600, 'completed')\n\n        # Log the completion of the task\n        end_time = time.perf_counter()\n        logging.info(f'Segment batch created job: {job_id} latency: {end_time - start_time}')\n\n    except Exception as e:\n        # Log the error and set the cache key to indicate the task has failed\n        logging.exception(f\"Segments batch created index failed: {e}\")\n        redis_client.setex(indexing_cache_key, 600, 'error')\n```\n\nThis high-level pythonic pseudocode follows the same logic as the provided code, but it is more abstract and informative. The comments explain the purpose of each section of the code, making it easier to understand the overall functionality of the task.\n\nThe key steps are:\n\n1. Define a Celery shared task for batch creating segments to index.\n2. Log the start of the task and define a cache key for the indexing task.\n3. Fetch the dataset and document from the database, and validate their availability.\n4. Initialize the embedding model if the dataset uses high-quality indexing.\n5. Create the document segments, calculate the embeddings, and add them to the database.\n6. Add the segments to the index using the IndexingRunner.\n7. Commit the changes to the database and set the cache key to indicate the task has completed.\n8. Log the completion of the task.\n9. Handle any exceptions that may occur, log the error, and set the cache key to indicate the task has failed.",
    "import_relationships": "Imports found:\nimport datetime\nimport logging\nimport time\nimport uuid\nfrom typing import cast\nimport click\nfrom celery import shared_task\nfrom sqlalchemy import func\nfrom core.indexing_runner import IndexingRunner\nfrom core.model_manager import ModelManager\nfrom core.model_runtime.entities.model_entities import ModelType\nfrom core.model_runtime.model_providers.__base.text_embedding_model import TextEmbeddingModel\nfrom extensions.ext_database import db\nfrom extensions.ext_redis import redis_client\nfrom libs import helper\nfrom models.dataset import Dataset, Document, DocumentSegment"
}