{
    "highlights": "The key features of this code are:\n\n1. **API Endpoints**: The code defines a wide range of API endpoints for managing datasets, documents, segments, hit testing, and other related functionality. These endpoints cover create, read, update, and delete operations for various entities.\n\n2. **Typed Interfaces**: The code extensively uses TypeScript type definitions to define the request and response payloads for the API endpoints. This ensures type safety and better maintainability of the codebase.\n\n3. **Fetcher Functions**: The code defines a set of reusable `Fetcher` functions that encapsulate the logic for making API requests using the `swr` library. These functions abstract away the details of the HTTP requests and provide a consistent interface for interacting with the API.\n\n4. **Batch Processing**: The code includes functions for handling batch-level operations, such as fetching indexing estimates and statuses for a batch of documents.\n\n5. **Utility Functions**: The code includes some utility functions, such as `fetchDatasetApiBaseUrl` and `fetchSupportFileTypes`, which provide additional functionality for working with the datasets and related data.\n\nOverall, this code appears to be a part of a larger application that manages datasets and related content. The extensive use of TypeScript types, reusable fetcher functions, and support for batch processing suggest a well-designed and maintainable codebase.",
    "overall_summary": "This codebase appears to be a set of API functions for interacting with a dataset management system. Here's an overall summary:\n\n1. **Dataset Management**:\n   - Functions to fetch, create, update, and delete datasets.\n   - Functions to fetch related apps for a dataset.\n   - Functions to fetch default and custom process rules for a dataset.\n\n2. **Document Management**:\n   - Functions to fetch, create, and delete documents within a dataset.\n   - Functions to fetch document details, indexing status, and indexing estimates.\n   - Functions to pause, resume, archive, unarchive, enable, and disable documents.\n   - Functions to synchronize documents with Notion.\n   - Functions to modify document metadata.\n\n3. **Segment Management**:\n   - Functions to fetch, enable, disable, update, and delete segments within a document.\n   - Functions to batch import segments.\n   - Functions to check the progress of a segment batch import.\n\n4. **Hit Testing**:\n   - Functions to perform hit testing on a dataset.\n   - Functions to fetch hit testing records.\n\n5. **Indexing Estimation**:\n   - Functions to fetch file indexing estimates.\n\n6. **API Key Management**:\n   - Functions to fetch, create, and delete API keys.\n\n7. **Miscellaneous**:\n   - Functions to fetch the API base URL for a dataset.\n   - Functions to fetch the supported file types.\n\nThe codebase utilizes the `swr` library for fetching data and the `qs` library for handling query string parameters. It also imports various types and models from other parts of the application.\n\nOverall, this codebase provides a comprehensive set of functions for managing datasets, documents, segments, and related functionality within a document management and search system.",
    "pseudocode": "```python\n# Import necessary modules and types\nimport requests\nfrom typing import Tuple, Dict, List, Union\n\n# Define common types for API requests and responses\nCommonDocReq = Dict[str, str]\nBatchReq = Dict[str, str]\nSortType = Union['created_at', 'hit_count', '-created_at', '-hit_count']\nMetadataType = Union['all', 'only', 'without']\nCreateDocumentReq = Dict[str, Any]\nDataSet = Dict[str, Any]\nDataSetListResponse = Dict[str, Any]\nDocumentDetailResponse = Dict[str, Any]\nDocumentListResponse = Dict[str, Any]\nFileIndexingEstimateResponse = Dict[str, Any]\nHitTestingRecordsResponse = Dict[str, Any]\nHitTestingResponse = Dict[str, Any]\nIndexingEstimateParams = Dict[str, Any]\nIndexingEstimateResponse = Dict[str, Any]\nIndexingStatusBatchResponse = Dict[str, Any]\nIndexingStatusResponse = Dict[str, Any]\nProcessRuleResponse = Dict[str, Any]\nRelatedAppResponse = Dict[str, Any]\nSegmentDetailModel = Dict[str, Any]\nSegmentUpdator = Dict[str, Any]\nSegmentsQuery = Dict[str, Any]\nSegmentsResponse = Dict[str, Any]\ncreateDocumentResponse = Dict[str, Any]\nCommonResponse = Dict[str, Any]\nDataSourceNotionWorkspace = Dict[str, Any]\nApikeysListResponse = Dict[str, Any]\nCreateApiKeyResponse = Dict[str, Any]\nRetrievalConfig = Dict[str, Any]\nFileTypesRes = Dict[str, Any]\n\n# Define API functions\ndef fetch_dataset_detail(dataset_id: str) -> DataSet:\n    \"\"\"Fetch details of a dataset\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}\").json()\n\ndef update_dataset_setting(dataset_id: str, body: Dict[str, Any]) -> DataSet:\n    \"\"\"Update settings of a dataset\"\"\"\n    return requests.patch(f\"/datasets/{dataset_id}\", json=body).json()\n\ndef fetch_dataset_related_apps(dataset_id: str) -> RelatedAppResponse:\n    \"\"\"Fetch related apps for a dataset\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/related-apps\").json()\n\ndef fetch_datasets(url: str, params: Dict[str, Any]) -> DataSetListResponse:\n    \"\"\"Fetch a list of datasets\"\"\"\n    return requests.get(f\"{url}?{urlencode(params, doseq=False)}\").json()\n\ndef create_empty_dataset(name: str) -> DataSet:\n    \"\"\"Create a new empty dataset\"\"\"\n    return requests.post(\"/datasets\", json={\"name\": name}).json()\n\ndef delete_dataset(dataset_id: str) -> DataSet:\n    \"\"\"Delete a dataset\"\"\"\n    return requests.delete(f\"/datasets/{dataset_id}\").json()\n\ndef fetch_default_process_rule(url: str) -> ProcessRuleResponse:\n    \"\"\"Fetch the default process rule\"\"\"\n    return requests.get(url).json()\n\ndef fetch_process_rule(document_id: str) -> ProcessRuleResponse:\n    \"\"\"Fetch the process rule for a document\"\"\"\n    return requests.get(\"/datasets/process-rule\", params={\"document_id\": document_id}).json()\n\ndef fetch_documents(dataset_id: str, params: Dict[str, Any]) -> DocumentListResponse:\n    \"\"\"Fetch a list of documents in a dataset\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/documents\", params=params).json()\n\ndef create_first_document(body: CreateDocumentReq) -> createDocumentResponse:\n    \"\"\"Create the first document in a dataset\"\"\"\n    return requests.post(\"/datasets/init\", json=body).json()\n\ndef create_document(dataset_id: str, body: CreateDocumentReq) -> createDocumentResponse:\n    \"\"\"Create a new document in a dataset\"\"\"\n    return requests.post(f\"/datasets/{dataset_id}/documents\", json=body).json()\n\ndef fetch_indexing_estimate(dataset_id: str, document_id: str) -> IndexingEstimateResponse:\n    \"\"\"Fetch the indexing estimate for a document\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/documents/{document_id}/indexing-estimate\").json()\n\ndef fetch_indexing_estimate_batch(dataset_id: str, batch_id: str) -> IndexingEstimateResponse:\n    \"\"\"Fetch the indexing estimate for a batch of documents\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/batch/{batch_id}/indexing-estimate\").json()\n\ndef fetch_indexing_status(dataset_id: str, document_id: str) -> IndexingStatusResponse:\n    \"\"\"Fetch the indexing status for a document\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/documents/{document_id}/indexing-status\").json()\n\ndef fetch_indexing_status_batch(dataset_id: str, batch_id: str) -> IndexingStatusBatchResponse:\n    \"\"\"Fetch the indexing status for a batch of documents\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/batch/{batch_id}/indexing-status\").json()\n\ndef fetch_document_detail(dataset_id: str, document_id: str, params: Dict[str, Any]) -> DocumentDetailResponse:\n    \"\"\"Fetch the details of a document\"\"\"\n    return requests.get(f\"/datasets/{dataset_id}/documents/{document_id}\", params=params).json()\n\ndef pause_doc_indexing(dataset_id: str, document_id: str) -> CommonResponse:\n    \"\"\"Pause the indexing of a document\"\"\"\n    return requests.patch(f\"/datasets/{dataset_id}/documents/{document_id}/processing/pause\").json()\n\ndef resume_doc_indexing(dataset_id: str, document_id: str) -> CommonResponse:\n    \"\"\"Resume the indexing of a document\"\"\"\n    return requests.patch(f\"/datasets/{dataset_id}/documents/{document_id}/processing/resume\").json()\n\ndef delete_document(dataset_id: str, document_id: str) -> CommonResponse:\n    \"\"\"Delete a document\"\"\"\n    return requests.delete(f\"/datasets/{dataset_id}/documents/{document_id}\").json()\n\n# and so on, with similar functions for other API endpoints\n```\n\nThis pseudocode provides a high-level overview of the API functions available in the `datasets.ts` file. It defines the common types used for API requests and responses, and then provides a set of function definitions that abstract away the underlying implementation details. Each function has a descriptive name and a brief docstring that explains its purpose. The code is written in a Pythonic style, using type annotations and Python's built-in `requests` library for making HTTP requests.",
    "import_relationships": "Imports found:\nimport type { Fetcher } from 'swr'\nimport qs from 'qs'\nimport { del, get, patch, post, put } from './base'\nimport type {\nimport type { CommonResponse, DataSourceNotionWorkspace } from '@/models/common'\nimport type {\nimport type { RetrievalConfig } from '@/types/app'"
}