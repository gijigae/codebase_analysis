{
    "highlights": "The key features of this code are:\n\n1. **Asynchronous Task**: The code uses a Celery shared task called `deal_dataset_vector_index_task` to handle the dataset vector indexing asynchronously. This allows the task to be executed in the background, freeing up the main application to handle other requests.\n\n2. **Dataset Management**: The code interacts with the `Dataset` and `DatasetDocument` models to retrieve and process the dataset information. It checks if the dataset exists and handles two actions: \"remove\" to clean the index, and \"add\" to load the dataset documents into the index.\n\n3. **Index Processor**: The code uses the `IndexProcessorFactory` to initialize the appropriate index processor based on the dataset's document form. This allows the code to handle different indexing strategies based on the dataset configuration.\n\n4. **Document Handling**: When adding documents to the index, the code retrieves the individual document segments, creates `Document` objects with the necessary metadata, and then loads them into the index using the `index_processor.load()` method.\n\n5. **Error Handling**: The code wraps the main functionality in a try-except block to catch any exceptions that may occur during the indexing process and logs the error message.\n\nThe key aspect of this code is the ability to handle the asynchronous indexing of dataset documents, with support for both adding and removing documents from the index. The use of the `IndexProcessorFactory` and the interaction with the dataset models demonstrate a well-structured and modular approach to the problem.",
    "overall_summary": "Here is an overall summary of the codebase in the `deal_dataset_vector_index_task.py` file:\n\n1. **Imports**: The file imports various libraries and modules, including `logging`, `time`, `click`, `celery`, and several custom modules/classes from the project's codebase.\n\n2. **deal_dataset_vector_index_task(dataset_id, action)**: This is a Celery shared task that handles the asynchronous processing of dataset vector indexing. It takes two parameters:\n   - `dataset_id`: the ID of the dataset to be processed.\n   - `action`: the action to be performed, either \"remove\" or \"add\".\n\n3. **Task Logic**:\n   - The task first logs a message and records the start time.\n   - It then retrieves the dataset from the database based on the provided `dataset_id`.\n   - If the dataset is not found, an exception is raised.\n   - Based on the `action` parameter, the task either:\n     - Removes the dataset from the vector index using the `IndexProcessorFactory` and `IndexProcessor.clean()` method.\n     - Adds the dataset to the vector index by:\n       - Retrieving all the `DatasetDocument` records for the dataset that have a \"completed\" indexing status, are enabled, and are not archived.\n       - Creating `Document` objects for each document segment associated with the dataset documents and adding them to a list.\n       - Using the `IndexProcessorFactory` and `IndexProcessor.load()` method to save the documents to the vector index.\n   - Finally, the task logs the latency of the operation.\n\n4. **Exception Handling**: The task is wrapped in a try-except block to handle any exceptions that may occur during the execution of the task. If an exception is raised, it is logged using the `logging.exception()` function.\n\nIn summary, this codebase provides a Celery shared task that handles the asynchronous processing of dataset vector indexing, allowing for the addition or removal of datasets from the vector index.",
    "pseudocode": "```python\n# Define a Celery task to handle dataset vector index operations\n@shared_task(queue='dataset')\ndef deal_dataset_vector_index_task(dataset_id: str, action: str):\n    \"\"\"\n    Asynchronously handle dataset vector index operations.\n    \n    Args:\n        dataset_id (str): The ID of the dataset.\n        action (str): The action to perform, either \"remove\" or \"add\".\n    \"\"\"\n    # Log the start of the task\n    logging.info(f\"Start dealing dataset vector index: {dataset_id}\")\n    start_time = time.perf_counter()\n\n    try:\n        # Fetch the dataset\n        dataset = Dataset.query.filter_by(id=dataset_id).first()\n\n        # Raise an exception if the dataset is not found\n        if not dataset:\n            raise Exception(\"Dataset not found\")\n\n        # Get the index type from the dataset\n        index_type = dataset.doc_form\n\n        # Initialize the index processor\n        index_processor = IndexProcessorFactory(index_type).init_index_processor()\n\n        # Perform the requested action\n        if action == \"remove\":\n            # Remove the dataset from the index\n            index_processor.clean(dataset, None, with_keywords=False)\n        elif action == \"add\":\n            # Fetch the completed, enabled, and non-archived dataset documents\n            dataset_documents = (\n                db.session.query(DatasetDocument)\n                .filter(\n                    DatasetDocument.dataset_id == dataset_id,\n                    DatasetDocument.indexing_status == 'completed',\n                    DatasetDocument.enabled == True,\n                    DatasetDocument.archived == False,\n                )\n                .all()\n            )\n\n            # Process each dataset document\n            if dataset_documents:\n                documents = []\n                for dataset_document in dataset_documents:\n                    # Fetch the document segments\n                    segments = (\n                        db.session.query(DocumentSegment)\n                        .filter(\n                            DocumentSegment.document_id == dataset_document.id,\n                            DocumentSegment.enabled == True,\n                        )\n                        .order_by(DocumentSegment.position.asc())\n                        .all()\n                    )\n\n                    # Create a Document object for each segment and add it to the list\n                    for segment in segments:\n                        document = Document(\n                            page_content=segment.content,\n                            metadata={\n                                \"doc_id\": segment.index_node_id,\n                                \"doc_hash\": segment.index_node_hash,\n                                \"document_id\": segment.document_id,\n                                \"dataset_id\": segment.dataset_id,\n                            },\n                        )\n                        documents.append(document)\n\n                # Load the documents into the vector index\n                index_processor.load(dataset, documents, with_keywords=False)\n\n        # Log the completion of the task and the latency\n        end_time = time.perf_counter()\n        logging.info(f\"Dealt dataset vector index: {dataset_id}, latency: {end_time - start_time}\")\n    except Exception:\n        # Log any exceptions that occurred during the task\n        logging.exception(\"Deal dataset vector index failed\")\n```\n\nThe high-level pseudocode above represents the main logic of the `deal_dataset_vector_index_task` function. The key steps are:\n\n1. Log the start of the task and record the start time.\n2. Fetch the dataset from the database using the provided `dataset_id`.\n3. If the dataset is not found, raise an exception.\n4. Get the index type from the dataset.\n5. Initialize the index processor using the index type.\n6. Perform the requested action:\n   - If the action is \"remove\", remove the dataset from the index.\n   - If the action is \"add\", fetch the completed, enabled, and non-archived dataset documents, create `Document` objects for each document segment, and load the documents into the vector index.\n7. Log the completion of the task and the latency.\n8. If any exceptions occur during the task, log the exception.\n\nThe pseudocode is written in a high-level, abstract manner, focusing on the overall logic and structure of the function rather than specific implementation details. This provides a clear and informative overview of the task's functionality.",
    "import_relationships": "Imports found:\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom core.rag.index_processor.index_processor_factory import IndexProcessorFactory\nfrom core.rag.models.document import Document\nfrom extensions.ext_database import db\nfrom models.dataset import Dataset, DocumentSegment\nfrom models.dataset import Document as DatasetDocument"
}