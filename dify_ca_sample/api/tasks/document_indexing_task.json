{
    "highlights": "Here are the key features of the provided code:\n\n1. **Asynchronous Task**: The code defines a Celery shared task called `document_indexing_task` that is responsible for processing documents asynchronously.\n\n2. **Database Interactions**: The code interacts with the database using the `db` object from the `extensions.ext_database` module. It queries the `Dataset` and `Document` models to fetch data and update the indexing status of documents.\n\n3. **Feature Checks**: The code checks the features of the dataset's tenant, specifically the `billing.enabled` and `vector_space.limit` settings, to ensure that the number of documents to be indexed does not exceed the limits.\n\n4. **Document Indexing**: The code creates an `IndexingRunner` instance and calls its `run` method to perform the actual indexing of the documents.\n\n5. **Error Handling**: The code catches exceptions, such as `DocumentIsPausedException`, and updates the indexing status and error message of the documents accordingly.\n\nOverall, the key focus of this code is to handle the asynchronous indexing of documents, while ensuring that the indexing process adheres to the tenant's feature limitations and properly handles any errors that may occur during the indexing process.",
    "overall_summary": "This Python file `document_indexing_task.py` is responsible for handling the asynchronous document indexing process within a codebase. Here's an overall summary of the codebase:\n\n1. **Imports**: The file imports various modules and classes, including `datetime`, `logging`, `time`, `click`, `celery`, `flask`, `indexing_runner`, `ext_database`, `models`, and `services`.\n\n2. **`document_indexing_task` function**: This function is a Celery shared task that runs asynchronously. It takes a `dataset_id` and a list of `document_ids` as input parameters. The main functionality of this task is as follows:\n   - Retrieves the dataset and the associated features based on the given `dataset_id`.\n   - Checks the document limit based on the features and the current app's configuration. If the limit is exceeded, it marks the documents as having an error and returns.\n   - Retrieves the documents from the database based on the `document_ids` and updates their indexing status to 'parsing'.\n   - Creates an `IndexingRunner` instance and runs the indexing process for the retrieved documents.\n   - Logs the processing time for the dataset.\n   - Handles exceptions, such as `DocumentIsPausedException`, and updates the document status accordingly.\n\n3. **Usage**: The `document_indexing_task` function is meant to be called asynchronously, for example, using `document_indexing_task.delay(dataset_id, document_id)`.\n\n4. **Dependencies**: The codebase relies on several external libraries and services, including Celery, Flask, the database layer (`ext_database` and `models`), and the `FeatureService` from the `services` module.\n\n5. **Error Handling**: The codebase handles various exceptions, such as `DocumentIsPausedException` and general exceptions, and updates the document status accordingly.\n\nOverall, this codebase is responsible for the asynchronous indexing of documents within a dataset, with considerations for document limits and error handling.",
    "pseudocode": "Here's the high-level pythonic pseudocode for the provided code:\n\n```python\n# Import necessary modules and dependencies\nimport datetime\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom flask import current_app\nfrom core.indexing_runner import DocumentIsPausedException, IndexingRunner\nfrom extensions.ext_database import db\nfrom models.dataset import Dataset, Document\nfrom services.feature_service import FeatureService\n\n# Define the document_indexing_task function as a Celery shared task\n@shared_task(queue='dataset')\ndef document_indexing_task(dataset_id: str, document_ids: list):\n    \"\"\"\n    Asynchronously process documents for a given dataset.\n\n    Args:\n        dataset_id (str): The ID of the dataset.\n        document_ids (list): A list of document IDs to be processed.\n    \"\"\"\n    # Initialize a list to store the documents\n    documents = []\n\n    # Record the start time of the task\n    start_at = time.perf_counter()\n\n    # Fetch the dataset from the database\n    dataset = db.session.query(Dataset).filter(Dataset.id == dataset_id).first()\n\n    # Check the document limit based on the dataset's tenant features\n    try:\n        features = FeatureService.get_features(dataset.tenant_id)\n        if features.billing.enabled:\n            vector_space = features.vector_space\n            count = len(document_ids)\n            batch_upload_limit = int(current_app.config['BATCH_UPLOAD_LIMIT'])\n            if count > batch_upload_limit:\n                raise ValueError(f\"You have reached the batch upload limit of {batch_upload_limit}.\")\n            if 0 < vector_space.limit <= vector_space.size:\n                raise ValueError(\"Your total number of documents plus the number of uploads have over the limit of your subscription.\")\n    except Exception as e:\n        # Handle the error by updating the indexing status and error message for each document\n        for document_id in document_ids:\n            document = db.session.query(Document).filter(\n                Document.id == document_id,\n                Document.dataset_id == dataset_id\n            ).first()\n            if document:\n                document.indexing_status = 'error'\n                document.error = str(e)\n                document.stopped_at = datetime.datetime.utcnow()\n                db.session.add(document)\n        db.session.commit()\n        return\n\n    # Fetch the documents from the database and update their indexing status\n    for document_id in document_ids:\n        logging.info(click.style(f'Start process document: {document_id}', fg='green'))\n        document = db.session.query(Document).filter(\n            Document.id == document_id,\n            Document.dataset_id == dataset_id\n        ).first()\n        if document:\n            document.indexing_status = 'parsing'\n            document.processing_started_at = datetime.datetime.utcnow()\n            documents.append(document)\n            db.session.add(document)\n    db.session.commit()\n\n    # Process the documents using the IndexingRunner\n    try:\n        indexing_runner = IndexingRunner()\n        indexing_runner.run(documents)\n        end_at = time.perf_counter()\n        logging.info(click.style(f'Processed dataset: {dataset_id} latency: {end_at - start_at}', fg='green'))\n    except DocumentIsPausedException as ex:\n        logging.info(click.style(str(ex), fg='yellow'))\n    except Exception:\n        pass\n```\n\nThe key aspects of this pseudocode are:\n\n1. The `document_indexing_task` function is defined as a Celery shared task, which allows it to be executed asynchronously.\n2. The function takes a `dataset_id` and a list of `document_ids` as input.\n3. It fetches the dataset from the database and checks the document limit based on the dataset's tenant features.\n4. If the document limit is exceeded, it updates the indexing status and error message for each document and returns.\n5. Otherwise, it fetches the documents from the database, updates their indexing status, and appends them to the `documents` list.\n6. The `IndexingRunner` is then used to process the documents.\n7. If a `DocumentIsPausedException` is raised, it is logged with a yellow color. Any other exceptions are caught and ignored.\n8. The total processing time is logged with a green color.\n\nThe pseudocode provides a high-level overview of the functionality without getting into the implementation details of each step. It highlights the key responsibilities of the `document_indexing_task` function, such as fetching the dataset, checking the document limit, updating the indexing status, and processing the documents using the `IndexingRunner`.",
    "import_relationships": "Imports found:\nimport datetime\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom flask import current_app\nfrom core.indexing_runner import DocumentIsPausedException, IndexingRunner\nfrom extensions.ext_database import db\nfrom models.dataset import Dataset, Document\nfrom services.feature_service import FeatureService"
}