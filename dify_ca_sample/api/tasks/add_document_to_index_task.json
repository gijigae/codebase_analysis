{
    "highlights": "The key features of this code are:\n\n1. **Asynchronous Task**: The code uses Celery's `@shared_task` decorator to define an asynchronous task called `add_document_to_index_task`. This allows the task to be executed in the background, independent of the main application flow.\n\n2. **Database Operations**: The code interacts with the database using SQLAlchemy, querying the `DatasetDocument` and `DocumentSegment` models to retrieve the necessary data for indexing.\n\n3. **Document Indexing**: The code uses the `IndexProcessorFactory` to initialize an appropriate index processor based on the dataset's `doc_form` attribute. The documents are then loaded into the index.\n\n4. **Error Handling**: The code includes error handling, where if an exception occurs during the indexing process, the `DatasetDocument` is marked as disabled and the error is logged.\n\n5. **Caching**: The code uses a Redis cache key to track the indexing status of the document, and this key is deleted after the indexing is complete.\n\nThe overall purpose of this code is to asynchronously add a document to a search index, with error handling and caching mechanisms in place to ensure reliable and efficient indexing.",
    "overall_summary": "The code you provided is an asynchronous task for adding a document to an index. Here's a summary of the key elements:\n\n1. **add_document_to_index_task**: This is a Celery shared task that is responsible for the asynchronous processing of adding a document to an index. It takes a `dataset_document_id` as a parameter.\n\n2. **Workflow**:\n   - It retrieves the `DatasetDocument` object from the database based on the `dataset_document_id`.\n   - If the `indexing_status` of the `DatasetDocument` is not \"completed\", the task returns without doing anything.\n   - It creates an indexing cache key in Redis to track the indexing process.\n   - It retrieves all the `DocumentSegment` objects associated with the `DatasetDocument` and creates a list of `Document` objects from the segment contents.\n   - It loads the documents into the index processor, which is determined by the `doc_form` field of the dataset associated with the `DatasetDocument`.\n   - If any exceptions occur during the process, it updates the `DatasetDocument` object with the error information and disables the document.\n   - Finally, it deletes the indexing cache key from Redis.\n\n3. **Dependencies**:\n   - The task uses several external libraries and modules, including `click`, `celery`, `werkzeug`, `core.rag.index_processor.index_processor_factory`, `core.rag.models.document`, `extensions.ext_database`, `extensions.ext_redis`, `models.dataset`, and `models.dataset`.\n   - It interacts with the database and Redis to retrieve and update data.\n\nOverall, this code is responsible for the asynchronous processing of adding a document to an index, handling potential errors, and updating the database and Redis accordingly.",
    "pseudocode": "Here is the high-level pythonic pseudocode for the `add_document_to_index_task` function:\n\n```python\n# Define a Celery shared task to add a document to the index asynchronously\n@shared_task(queue='dataset')\ndef add_document_to_index_task(dataset_document_id: str):\n    \"\"\"\n    Asynchronously add a document to the index.\n\n    Args:\n        dataset_document_id (str): The ID of the dataset document to be added to the index.\n\n    Usage:\n        add_document_to_index.delay(document_id)\n    \"\"\"\n    # Log the start of the task and record the start time\n    logging.info(f\"Start adding document to index: {dataset_document_id}\")\n    start_at = time.perf_counter()\n\n    # Fetch the dataset document from the database\n    dataset_document = db.session.query(DatasetDocument).filter(DatasetDocument.id == dataset_document_id).first()\n    if not dataset_document:\n        # Raise an exception if the document is not found\n        raise NotFound(\"Document not found\")\n\n    # Check if the document is ready to be indexed\n    if dataset_document.indexing_status != \"completed\":\n        return\n\n    # Get the indexing cache key for the document\n    indexing_cache_key = f\"document_{dataset_document.id}_indexing\"\n\n    try:\n        # Fetch all the enabled segments of the document from the database\n        segments = db.session.query(DocumentSegment).filter(\n            DocumentSegment.document_id == dataset_document.id,\n            DocumentSegment.enabled == True\n        ).order_by(DocumentSegment.position.asc()).all()\n\n        # Create a list of document objects from the segments\n        documents = []\n        for segment in segments:\n            document = Document(\n                page_content=segment.content,\n                metadata={\n                    \"doc_id\": segment.index_node_id,\n                    \"doc_hash\": segment.index_node_hash,\n                    \"document_id\": segment.document_id,\n                    \"dataset_id\": segment.dataset_id,\n                }\n            )\n            documents.append(document)\n\n        # Fetch the dataset associated with the document\n        dataset = dataset_document.dataset\n        if not dataset:\n            # Raise an exception if the document has no associated dataset\n            raise Exception(\"Document has no dataset\")\n\n        # Get the index type from the dataset\n        index_type = dataset.doc_form\n\n        # Create an index processor based on the index type\n        index_processor = IndexProcessorFactory(index_type).init_index_processor()\n\n        # Load the documents into the index\n        index_processor.load(dataset, documents)\n\n        # Log the completion of the task and the latency\n        end_at = time.perf_counter()\n        logging.info(\n            f\"Document added to index: {dataset_document.id} latency: {end_at - start_at}\"\n        )\n    except Exception as e:\n        # Log the exception and update the document status\n        logging.exception(\"add document to index failed\")\n        dataset_document.enabled = False\n        dataset_document.disabled_at = datetime.datetime.utcnow()\n        dataset_document.status = \"error\"\n        dataset_document.error = str(e)\n        db.session.commit()\n    finally:\n        # Delete the indexing cache key\n        redis_client.delete(indexing_cache_key)\n```\n\nThis high-level pseudocode provides a clear and concise overview of the logic implemented in the `add_document_to_index_task` function. It includes comments that explain the purpose of each section of the code, making it easy for developers to understand the overall flow of the function.\n\nThe pseudocode follows Python conventions and best practices, such as using descriptive variable names, handling exceptions, and logging relevant information. It also demonstrates the use of Celery shared tasks, database queries, and index processing, which are common components in a production-ready application.",
    "import_relationships": "Imports found:\nimport datetime\nimport logging\nimport time\nimport click\nfrom celery import shared_task\nfrom werkzeug.exceptions import NotFound\nfrom core.rag.index_processor.index_processor_factory import IndexProcessorFactory\nfrom core.rag.models.document import Document\nfrom extensions.ext_database import db\nfrom extensions.ext_redis import redis_client\nfrom models.dataset import Document as DatasetDocument\nfrom models.dataset import DocumentSegment"
}